# LM Studio Configuration
# Configure these settings based on your LM Studio setup

# LM Studio Server Configuration
llmstudio.base.url=http://localhost:1234
llmstudio.model.name=qwen/qwen3-1.7b
llmstudio.timeout.seconds=90

# Alternative configurations for different setups:
# llmstudio.base.url=http://127.0.0.1:1234
# llmstudio.model.name=your-model-name
# llmstudio.timeout.seconds=120

# Model Examples:
# - For Llama models: llama-2-7b-chat, llama-2-13b-chat
# - For Mistral models: mistral-7b-instruct, mistral-8x7b-instruct  
# - For CodeLlama models: codellama-7b-instruct, codellama-13b-instruct
# - For custom models: use the exact name shown in LM Studio

# Provider Priority (comma-separated)
# Options: OLLAMA, LLMSTUDIO, SIMPLE
ai.provider.priority=OLLAMA,LLMSTUDIO,SIMPLE

# Enable/Disable fallback to other providers
ai.fallback.enabled=true

# AI Generation Parameters
ai.temperature.default=0.7
ai.max.tokens.default=200
ai.temperature.creative=0.8
ai.temperature.precise=0.1

# Timeout settings for different operations
ai.quick.test.timeout=30
ai.generation.timeout=90
ai.diagnostics.timeout=20